{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erikaduda/dsto-gan/blob/main/DSTO_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PieHdynSTjch"
      },
      "source": [
        "**DSTO GAN**\n",
        "\n",
        "\n",
        "Para melhorar o algoritmo de balanceamento DeepSMOTE com GAN e obter as melhores análises de F1-Score, Recall e Precision, vamos considerar algumas mudanças e melhorias:\n",
        "\n",
        "1.Hiperparâmetros do Modelo : Ajusta os hiperparâmetros conforme a dimensão do espaço latente ( n_z), número de épocas ( epochs) e taxas de\n",
        "Para melhorar o algoritmo de balanceamento DeepSMOTE com GAN e obter as melhores análises de F1-Score, Recall e Precision, vamos considerar algumas mudanças e melhorias:\n",
        "\n",
        "1.Hiperparâmetros do Modelo : Ajusta os hiperparâmetros conforme a dimensão do espaço latente ( n_z), número de épocas ( epochs) e taxas de aprendizado ( lr) para obter melhor desempenho.\n",
        "\n",
        "2.Arquitetura do Modelo : Experimentar com diferentes arquiteturas para o Encodere Decoder, como adicionar mais camadas ou utilizar diferentes funções de ativação.\n",
        "\n",
        "3.Aumento de dados : Inclui técnicas de aumento de dados para aumentar a variedade dos dados sintéticos gerados.\n",
        "\n",
        "4.Avaliação de Desempenho : Utilização diferentes classificados e fazer uma análise aprofundada das métricas para\n",
        "\n",
        "Essa versão do código inclui um Discriminador para formar um GAN, o que deve melhorar a qualidade das amostras sintéticas geradas. O treinamento do GAN é realizado em cada época, alternando entre o treinamento do Discriminador e do Gerador. As amostras sintéticas são geradas pelo Decoder do GAN, o que deve ajudar a melhorar as métricas de avaliação dos classificados.\n",
        "\n",
        "\n",
        "\n",
        "**Melhorias Implementadas:**\n",
        "\n",
        "1.Arquitetura do GAN: Adicionada uma rede Discriminator para melhorar a qualidade das amostras sintéticas geradas pelo Decoder.\n",
        "\n",
        "2.Função G_SM1: Agora usa o GAN para gerar amostras sintéticas.\n",
        "\n",
        "3.Treinamento do GAN: Implementado o treinamento do GAN, ajustando o gerador e o discriminador.\n",
        "\n",
        "4.Classificadores: Adicionado o XGBoost aos classificadores.\n",
        "\n",
        "5.Pipeline de Validação Cruzada: Implementada validação cruzada estratificada e coleta de métricas.\n",
        "\n",
        "6.Salvamento e Carregamento de Modelos:Ajustado o salvamento e carregamento dos modelos GAN e dos classificadores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JS13Vu9Q9Oe",
        "outputId": "8fc4cb71-4d55-4483-d7a4-b1614dde694b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L4MY6VGRBhT",
        "outputId": "33eae8fe-5fa3-4cec-b347-f43260f247f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-2.0.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting colorama<0.5.0,>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.5.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (3.5.0)\n",
            "Downloading bayesian_optimization-2.0.0-py3-none-any.whl (30 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-2.0.0 colorama-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu22wxrxRJiu",
        "outputId": "e4e028d6-1269-4763-ddf4-af4cec1d426c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-24.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.5.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-24.9.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-24.9.0 scikit-optimize-0.10.2\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XkqxSyphoZ0",
        "outputId": "e462b6f3-73a0-495e-f339-bb697c3eaa9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.5.1+cu121\n",
            "Uninstalling torch-2.5.1+cu121:\n",
            "  Successfully uninstalled torch-2.5.1+cu121\n",
            "Found existing installation: torchvision 0.20.1+cu121\n",
            "Uninstalling torchvision-0.20.1+cu121:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu121\n",
            "Found existing installation: torchaudio 2.5.1+cu121\n",
            "Uninstalling torchaudio-2.5.1+cu121:\n",
            "  Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "Collecting torch\n",
            "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1 triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TiwQ1sy-HGS",
        "outputId": "52f7c4ba-e1cf-4e62-8bc7-04e89c804f85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: IT_customer_churn.csv\n",
            "CLASS DISTRIBUTION:\n",
            "class\n",
            "0    73.463013\n",
            "1    26.536987\n",
            "Name: proportion, dtype: float64\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-c4a840ed8087>:218: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  encoder.load_state_dict(torch.load(os.path.join(MODELO_DIR_DST, 'encoder_epoch99.pt')))\n",
            "<ipython-input-6-c4a840ed8087>:219: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder.load_state_dict(torch.load(os.path.join(MODELO_DIR_DST, 'decoder_epoch99.pt')))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "<ipython-input-6-c4a840ed8087>:266: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  reports_df_train = pd.concat([reports_df_train, pd.DataFrame({\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "<ipython-input-6-c4a840ed8087>:218: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  encoder.load_state_dict(torch.load(os.path.join(MODELO_DIR_DST, 'encoder_epoch99.pt')))\n",
            "<ipython-input-6-c4a840ed8087>:219: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder.load_state_dict(torch.load(os.path.join(MODELO_DIR_DST, 'decoder_epoch99.pt')))\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "<ipython-input-6-c4a840ed8087>:218: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  encoder.load_state_dict(torch.load(os.path.join(MODELO_DIR_DST, 'encoder_epoch99.pt')))\n",
            "<ipython-input-6-c4a840ed8087>:219: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder.load_state_dict(torch.load(os.path.join(MODELO_DIR_DST, 'decoder_epoch99.pt')))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "<ipython-input-6-c4a840ed8087>:218: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  encoder.load_state_dict(torch.load(os.path.join(MODELO_DIR_DST, 'encoder_epoch99.pt')))\n",
            "<ipython-input-6-c4a840ed8087>:219: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  decoder.load_state_dict(torch.load(os.path.join(MODELO_DIR_DST, 'decoder_epoch99.pt')))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import joblib\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "\n",
        "# Definir parâmetros para os classificadores\n",
        "param_spaces = {\n",
        "    'Decision Tree': {'max_depth': (3, 15)},\n",
        "    'Random Forest': {'n_estimators': (10, 100)},\n",
        "    'Neural Network': {'alpha': (1e-4, 1e-2, 'log-uniform')},\n",
        "    'KNN': {'n_neighbors': (3, 10)},\n",
        "    'XGBoost': {'learning_rate': (0.01, 0.3)}\n",
        "}\n",
        "\n",
        "\n",
        "# Definir argumentos para o modelo\n",
        "args = {\n",
        "    'dim_h': 64,\n",
        "    'n_z': 10,\n",
        "    'lr': 0.0002,\n",
        "    'epochs': 100,\n",
        "    'batch_size': 64,\n",
        "    'save': True,\n",
        "    'train': True\n",
        "}\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args, num_input_features):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        self.fc1 = nn.Linear(num_input_features, self.dim_h)\n",
        "        self.fc2 = nn.Linear(self.dim_h, self.dim_h)\n",
        "        self.fc_mean = nn.Linear(self.dim_h, self.n_z)\n",
        "        self.fc_logvar = nn.Linear(self.dim_h, self.n_z)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        mean = self.fc_mean(x)\n",
        "        logvar = self.fc_logvar(x)\n",
        "        return mean, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args, num_input_features):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        self.fc1 = nn.Linear(self.n_z, self.dim_h)\n",
        "        self.fc2 = nn.Linear(self.dim_h, self.dim_h)\n",
        "        self.fc_output = nn.Linear(self.dim_h, num_input_features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc_output(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_input_features):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_input_features, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "def G_SM1(X, y, n_to_sample, cl, encoder, decoder):\n",
        "    # Genera amostras sintéticas usando o GAN\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "    dataloader = torch.utils.data.DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=args['batch_size'], shuffle=True)\n",
        "\n",
        "    synthetic_data = []\n",
        "    for _ in range(n_to_sample):\n",
        "        z = torch.randn(1, args['n_z'])\n",
        "        synthetic_sample = decoder(z).detach().numpy()\n",
        "        synthetic_data.append(synthetic_sample)\n",
        "\n",
        "    synthetic_data = np.vstack(synthetic_data)\n",
        "    synthetic_labels = np.array([cl] * n_to_sample)\n",
        "    return synthetic_data, synthetic_labels\n",
        "\n",
        "def calculate_n_to_sample(y):\n",
        "    class_counts = np.bincount(y)\n",
        "    major_class_count = np.max(class_counts)\n",
        "    n_classes = len(class_counts)\n",
        "    n_to_sample_dict = {cl: major_class_count - class_counts[cl] for cl in range(n_classes)}\n",
        "    return n_to_sample_dict, major_class_count\n",
        "\n",
        "# Diretório de entrada e saída\n",
        "INPUT_DIR = \"/content/drive/MyDrive/PHD_new/dataset_tratado/fast/\"\n",
        "OUTPUT_DIR_TRAIN = \"/content/drive/MyDrive/PHD_new/resultados/dsto_gan/treino/\"\n",
        "OUTPUT_DIR_TEST = \"/content/drive/MyDrive/PHD_new/resultados/dsto_gan/teste/\"\n",
        "MODELO_DIR = \"/content/drive/MyDrive/PHD_new/resultados/dsto_gan/modelos/\"\n",
        "MODELO_DIR_DST = \"/content/drive/MyDrive/PHD_new/resultados/dsto_gan/modelos_dsto/\"\n",
        "\n",
        "# Verificar se o diretório de saída existe, se não, criar\n",
        "os.makedirs(OUTPUT_DIR_TRAIN, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR_TEST, exist_ok=True)\n",
        "os.makedirs(MODELO_DIR, exist_ok=True)\n",
        "os.makedirs(MODELO_DIR_DST, exist_ok=True)\n",
        "\n",
        "# Listar arquivos .csv no diretório de entrada\n",
        "csv_files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.csv')]\n",
        "\n",
        "# Iterar sobre os arquivos .csv\n",
        "for csv_file in csv_files:\n",
        "    input_path = os.path.join(INPUT_DIR, csv_file)\n",
        "    data = pd.read_csv(input_path)\n",
        "    X = data.drop('class', axis=1).values\n",
        "    y = data['class'].values\n",
        "    print(f\"Processing: {csv_file}\")\n",
        "    print(f\"CLASS DISTRIBUTION:\\n{data['class'].value_counts(normalize=True) * 100}\\n\")\n",
        "\n",
        "    # Dividir os dados em conjunto de treinamento e validação\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "    # Inicializar o DataFrame para armazenar as métricas de avaliação\n",
        "    reports_df_train = pd.DataFrame(columns=['Classifier', 'Class', 'Fold', 'Precision', 'Recall', 'F1-Score'])\n",
        "    reports_df_val = pd.DataFrame(columns=['Classifier', 'Class', 'Precision', 'Recall', 'F1-Score'])\n",
        "\n",
        "\n",
        "    # Define classifiers\n",
        "    classifiers = {\n",
        "        'Decision Tree': DecisionTreeClassifier(),\n",
        "        'Random Forest': RandomForestClassifier(),\n",
        "        'Neural Network': MLPClassifier(),\n",
        "        'KNN': KNeighborsClassifier(),\n",
        "        'XGBoost': XGBClassifier()  # Adicionando XGBClassifier\n",
        "    }\n",
        "\n",
        "    # Inicializar listas para armazenar as métricas de validação de cada classificador\n",
        "    precision_val_list = {classifier_name: [] for classifier_name in classifiers}\n",
        "    recall_val_list = {classifier_name: [] for classifier_name in classifiers}\n",
        "    f1_val_list = {classifier_name: [] for classifier_name in classifiers}\n",
        "\n",
        "    # Realizar validação cruzada estratificada de 10 folds\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
        "        X_fold_train, X_fold_test = X_train[train_index], X_train[test_index]\n",
        "        y_fold_train, y_fold_test = y_train[train_index], y_train[test_index]\n",
        "\n",
        "        # Treinar o GAN\n",
        "        if args['train']:\n",
        "            encoder = Encoder(args, X_fold_train.shape[1])\n",
        "            decoder = Decoder(args, X_fold_train.shape[1])\n",
        "            discriminator = Discriminator(X_fold_train.shape[1])\n",
        "            optimizer_g = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=args['lr'])\n",
        "            optimizer_d = optim.Adam(discriminator.parameters(), lr=args['lr'])\n",
        "            criterion_g = nn.MSELoss()\n",
        "            criterion_d = nn.BCELoss()\n",
        "\n",
        "            dataloader = torch.utils.data.DataLoader(torch.tensor(X_fold_train, dtype=torch.float32), batch_size=args['batch_size'], shuffle=True)\n",
        "\n",
        "            for epoch in range(args['epochs']):\n",
        "                for batch in dataloader:\n",
        "                    # Treinar o discriminador\n",
        "                    optimizer_d.zero_grad()\n",
        "                    real_labels = torch.ones(batch.size(0), 1)\n",
        "                    fake_labels = torch.zeros(batch.size(0), 1)\n",
        "\n",
        "                    outputs = discriminator(batch)\n",
        "                    d_loss_real = criterion_d(outputs, real_labels)\n",
        "\n",
        "                    z = torch.randn(batch.size(0), args['n_z'])\n",
        "                    fake_data = decoder(z)\n",
        "                    outputs = discriminator(fake_data.detach())\n",
        "                    d_loss_fake = criterion_d(outputs, fake_labels)\n",
        "\n",
        "                    d_loss = d_loss_real + d_loss_fake\n",
        "                    d_loss.backward()\n",
        "                    optimizer_d.step()\n",
        "\n",
        "                    # Treinar o gerador\n",
        "                    optimizer_g.zero_grad()\n",
        "                    outputs = discriminator(fake_data)\n",
        "                    g_loss = criterion_g(fake_data, batch) + criterion_d(outputs, real_labels)\n",
        "                    g_loss.backward()\n",
        "                    optimizer_g.step()\n",
        "\n",
        "                if args['save']:\n",
        "                    torch.save(encoder.state_dict(), os.path.join(MODELO_DIR_DST, f'encoder_epoch{epoch}.pt'))\n",
        "                    torch.save(decoder.state_dict(), os.path.join(MODELO_DIR_DST, f'decoder_epoch{epoch}.pt'))\n",
        "\n",
        "        encoder.load_state_dict(torch.load(os.path.join(MODELO_DIR_DST, 'encoder_epoch99.pt')))\n",
        "        decoder.load_state_dict(torch.load(os.path.join(MODELO_DIR_DST, 'decoder_epoch99.pt')))\n",
        "\n",
        "        y = y.astype(np.int64)\n",
        "        n_to_sample_dict, major_class_count = calculate_n_to_sample(y)\n",
        "\n",
        "        X_synthetic_list = []\n",
        "        y_synthetic_list = []\n",
        "        for cl, n_samples in n_to_sample_dict.items():\n",
        "            if n_samples > 0:\n",
        "                X_synthetic, y_synthetic = G_SM1(X, y, n_samples, cl, encoder, decoder)\n",
        "                X_synthetic_list.append(X_synthetic)\n",
        "                y_synthetic_list.append(y_synthetic)\n",
        "\n",
        "        if X_synthetic_list:\n",
        "            X_synthetic_combined = np.concatenate(X_synthetic_list, axis=0)\n",
        "            y_synthetic_combined = np.concatenate(y_synthetic_list, axis=0)\n",
        "            X_combined = np.vstack((X, X_synthetic_combined))\n",
        "            y_combined = np.hstack((y, y_synthetic_combined))\n",
        "        else:\n",
        "            X_combined = X\n",
        "            y_combined = y\n",
        "\n",
        "        classifiers = {\n",
        "            'Decision Tree': DecisionTreeClassifier(),\n",
        "            'Random Forest': RandomForestClassifier(),\n",
        "            'Neural Network': MLPClassifier(),\n",
        "            'KNN': KNeighborsClassifier(),\n",
        "            'XGBoost': XGBClassifier()\n",
        "        }\n",
        "\n",
        "\n",
        "        optimized_classifiers = {}\n",
        "        for classifier_name, classifier in classifiers.items():\n",
        "            search = BayesSearchCV(classifier, param_spaces[classifier_name], n_iter=10, cv=3, n_jobs=-1, random_state=42)\n",
        "            search.fit(X_train, y_train)\n",
        "            optimized_classifiers[classifier_name] = search.best_estimator_\n",
        "\n",
        "        for classifier_name, classifier in optimized_classifiers.items():\n",
        "            classifier.fit(X_combined, y_combined)\n",
        "            joblib.dump(classifier, os.path.join(MODELO_DIR, f'{csv_file}_{classifier_name}_fold{fold}_model.pkl'))\n",
        "\n",
        "            y_pred_train = classifier.predict(X_combined)\n",
        "            precision_train = precision_score(y_combined, y_pred_train, average=None)\n",
        "            recall_train = recall_score(y_combined, y_pred_train, average=None)\n",
        "            f1_train = f1_score(y_combined, y_pred_train, average=None)\n",
        "\n",
        "            for class_label, precision, recall, f1 in zip(np.unique(y_combined), precision_train, recall_train, f1_train):\n",
        "                reports_df_train = pd.concat([reports_df_train, pd.DataFrame({\n",
        "                    'Classifier': classifier_name,\n",
        "                    'Class': class_label,\n",
        "                    'Fold': fold,\n",
        "                    'Precision': precision,\n",
        "                    'Recall': recall,\n",
        "                    'F1-Score': f1\n",
        "                }, index=[0])])\n",
        "\n",
        "            y_pred_val = classifier.predict(X_val)\n",
        "            precision_val = precision_score(y_val, y_pred_val, average=None)\n",
        "            recall_val = recall_score(y_val, y_pred_val, average=None)\n",
        "            f1_val = f1_score(y_val, y_pred_val, average=None)\n",
        "\n",
        "            precision_val_list[classifier_name].append(precision_val)\n",
        "            recall_val_list[classifier_name].append(recall_val)\n",
        "            f1_val_list[classifier_name].append(f1_val)\n",
        "\n",
        "    for classifier_name in classifiers:\n",
        "        precision_val_avg = np.mean(precision_val_list[classifier_name], axis=0)\n",
        "        recall_val_avg = np.mean(recall_val_list[classifier_name], axis=0)\n",
        "        f1_val_avg = np.mean(f1_val_list[classifier_name], axis=0)\n",
        "\n",
        "        for class_label, precision, recall, f1 in zip(np.unique(y), precision_val_avg, recall_val_avg, f1_val_avg):\n",
        "            reports_df_val = pd.concat([reports_df_val, pd.DataFrame({\n",
        "                'Classifier': classifier_name,\n",
        "                'Class': class_label,\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'F1-Score': f1\n",
        "            }, index=[0])])\n",
        "\n",
        "    reports_df_train.to_csv(os.path.join(OUTPUT_DIR_TRAIN, f'{csv_file}'), index=False)\n",
        "    reports_df_val.to_csv(os.path.join(OUTPUT_DIR_TEST, f'{csv_file}'), index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzBNQNwWq5sbh8q2s02mci",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}