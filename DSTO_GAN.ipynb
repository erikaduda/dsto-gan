{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erikaduda/dsto-gan/blob/main/DSTO_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PieHdynSTjch"
      },
      "source": [
        "**DSTO GAN**\n",
        "\n",
        "\n",
        "O algoritmo DSTO-GAN consiste em uma implementação adaptada do DeepSMOTE para dados tabulares, que integra um modelo DeepSMOTE com uma arquitetura de Rede Generativa Adversarial (GAN) para o balanceamento de conjuntos de dados. Especificamente, o DSTO-GAN emprega um Discriminador com o objetivo de aprimorar a qualidade das amostras sintéticas geradas durante o processo. O treinamento da GAN é conduzido de forma iterativa, alternando-se entre a otimização do Discriminador e do Gerador em cada época. As amostras sintéticas são produzidas pelo Decoder da GAN, que é responsável por mapear os dados do espaço latente para o espaço de características, garantindo a geração de exemplos sintéticos que preservam a distribuição original dos dados. Essa abordagem visa melhorar a representatividade das classes minoritárias, contribuindo para a eficácia de modelos de aprendizado de máquina em cenários desbalanceados.\n",
        "\n",
        "### Melhorias Implementadas no Algoritmo DeepSMOTE com GAN\n",
        "Neste trabalho, propõem-se uma série de melhorias ao algoritmo DeepSMOTE com GAN, visando aprimorar a qualidade das amostras sintéticas geradas, o desempenho do modelo e a robustez do processo de balanceamento de dados. As principais alterações e implementações são descritas a seguir:\n",
        "\n",
        "\n",
        "#### 1. Arquitetura do GAN\n",
        "Foi adicionada uma rede Discriminador à arquitetura GAN, com o objetivo de melhorar a qualidade das amostras sintéticas geradas pelo Decoder. O Discriminador atua como um crítico, avaliando a veracidade das amostras sintéticas em relação aos dados reais. Essa abordagem adversarial permite que o Gerador aprenda a produzir amostras mais realistas, aproximando-se da distribuição original dos dados. A interação entre o Gerador e o Discriminador é fundamental para garantir que as amostras sintéticas sejam representativas e úteis para o balanceamento de classes.\n",
        "\n",
        "#### 2. Função G_SM1\n",
        "A função G_SM1, responsável pela geração de amostras sintéticas, foi reformulada para incorporar o uso do GAN. Agora, em vez de depender exclusivamente de técnicas tradicionais de oversampling, a função utiliza o Gerador da GAN para criar amostras sintéticas a partir do espaço latente. Essa mudança permite a geração de dados mais diversificados e adaptados à distribuição dos dados reais, melhorando a eficácia do balanceamento.\n",
        "\n",
        "#### 3. Treinamento do GAN\n",
        "O treinamento da GAN foi implementado de forma iterativa, alternando entre a otimização do Gerador e do Discriminador. Em cada época, o Discriminador é treinado para distinguir entre amostras reais e sintéticas, enquanto o Gerador é ajustado para enganar o Discriminador, produzindo amostras cada vez mais realistas. Esse processo adversarial é repetido até que o Gerador consiga gerar amostras sintéticas de alta qualidade, que sejam indistinguíveis das reais pelo Discriminador.\n",
        "\n",
        "#### 4. Pipeline de Validação Cruzada\n",
        "Foi implementado um pipeline de validação cruzada estratificada para garantir uma avaliação robusta e imparcial do modelo. A validação cruzada estratificada preserva a proporção das classes em cada fold, evitando vieses na avaliação. Durante o processo, métricas como F1-Score, Recall e Precision são coletadas e analisadas, permitindo uma avaliação detalhada do desempenho do modelo em diferentes cenários.\n",
        "\n",
        "#### 5. Salvamento e Carregamento de Modelos\n",
        "O processo de salvamento e carregamento dos modelos GAN e dos classificadores foi ajustado para garantir a reprodutibilidade dos experimentos e a facilidade de uso em diferentes contextos. Agora, tanto o modelo GAN (incluindo o Gerador e o Discriminador) quanto os classificadores treinados podem ser salvos em arquivos e carregados posteriormente para inferência ou continuidade do treinamento. Essa funcionalidade é essencial para aplicações práticas, onde modelos pré-treinados podem ser reutilizados sem a necessidade de retreinamento.\n",
        "\n",
        "#### 6. Otimizações Adicionais para Maximização de Métricas\n",
        "Para otimizar ainda mais o algoritmo DeepSMOTE com GAN e maximizar as métricas de avaliação, como F1-Score, Recall e Precision, propõem-se as seguintes melhorias e ajustes:\n",
        "\n",
        "a) **Otimização de Hiperparâmetros**: Realizar ajustes finos nos hiperparâmetros do modelo, como a dimensão do espaço latente ((n_z)), o número de épocas de treinamento ((epochs)) e as taxas de aprendizado ((lr)), visando aprimorar o desempenho geral do algoritmo. A escolha adequada desses parâmetros é crucial para garantir a convergência e a eficácia do modelo.\n",
        "\n",
        "b) **Refinamento da Arquitetura do Modelo**: Explorar diferentes configurações arquiteturais para o Encoder e o Decoder, incluindo a adição de camadas adicionais ou a utilização de funções de ativação alternativas (e.g., ReLU, LeakyReLU, tanh). Essas modificações podem melhorar a capacidade de representação do modelo e a qualidade das amostras sintéticas geradas.\n",
        "\n",
        "c) **Técnicas de Aumento de Dados**: Incorporar métodos de aumento de dados para enriquecer a diversidade das amostras sintéticas. Isso pode incluir a aplicação de transformações ou perturbações controladas nos dados, aumentando a robustez do modelo e sua capacidade de generalização.\n",
        "\n",
        "d) **Avaliação de Desempenho**: Implementar uma avaliação abrangente utilizando diferentes classificadores (e.g., Random Forest, SVM, Redes Neurais) para analisar o impacto das amostras sintéticas geradas. Realizar uma análise detalhada das métricas de desempenho, como F1-Score, Recall e Precision, para validar a eficácia do balanceamento e a qualidade dos dados sintéticos.\n",
        "\n",
        "### Conclusão\n",
        "As melhorias implementadas no algoritmo DeepSMOTE com GAN visam aumentar a qualidade das amostras sintéticas geradas, melhorar o desempenho do modelo em tarefas de classificação e garantir a robustez do processo de balanceamento de dados. A adição do Discriminador, a reformulação da função G_SM1, o treinamento adversarial da GAN, a inclusão do XGBoost, a implementação de validação cruzada estratificada e o ajuste no salvamento e carregamento de modelos contribuem para um framework mais eficiente e aplicável em cenários reais. Além disso, as otimizações adicionais, como ajuste de hiperparâmetros, refinamento da arquitetura, técnicas de aumento de dados e avaliação de desempenho, permitem que o algoritmo lide de forma mais eficaz com conjuntos de dados desbalanceados, resultando em modelos de aprendizado de máquina mais precisos e generalizáveis. Essas alterações consolidam o DeepSMOTE com GAN como uma ferramenta robusta e versátil para o tratamento de dados desbalanceados em diversas aplicações."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JS13Vu9Q9Oe",
        "outputId": "8fc4cb71-4d55-4483-d7a4-b1614dde694b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L4MY6VGRBhT",
        "outputId": "33eae8fe-5fa3-4cec-b347-f43260f247f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-2.0.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting colorama<0.5.0,>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.5.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (3.5.0)\n",
            "Downloading bayesian_optimization-2.0.0-py3-none-any.whl (30 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-2.0.0 colorama-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu22wxrxRJiu",
        "outputId": "e4e028d6-1269-4763-ddf4-af4cec1d426c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-24.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.5.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-24.9.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-24.9.0 scikit-optimize-0.10.2\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XkqxSyphoZ0",
        "outputId": "e462b6f3-73a0-495e-f339-bb697c3eaa9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.5.1+cu121\n",
            "Uninstalling torch-2.5.1+cu121:\n",
            "  Successfully uninstalled torch-2.5.1+cu121\n",
            "Found existing installation: torchvision 0.20.1+cu121\n",
            "Uninstalling torchvision-0.20.1+cu121:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu121\n",
            "Found existing installation: torchaudio 2.5.1+cu121\n",
            "Uninstalling torchaudio-2.5.1+cu121:\n",
            "  Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "Collecting torch\n",
            "  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.5.1-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.5.1 torchaudio-2.5.1 torchvision-0.20.1 triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1TiwQ1sy-HGS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import joblib\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "\n",
        "# Definir parâmetros para os classificadores\n",
        "param_spaces = {\n",
        "    'Decision Tree': {'max_depth': (3, 15)},\n",
        "    'Random Forest': {'n_estimators': (10, 100)},\n",
        "    'Neural Network': {'alpha': (1e-4, 1e-2, 'log-uniform')},\n",
        "    'KNN': {'n_neighbors': (3, 10)},\n",
        "    'XGBoost': {'learning_rate': (0.01, 0.3)}\n",
        "}\n",
        "\n",
        "\n",
        "# Definir argumentos para o modelo\n",
        "args = {\n",
        "    'dim_h': 64,\n",
        "    'n_z': 10,\n",
        "    'lr': 0.0002,\n",
        "    'epochs': 100,\n",
        "    'batch_size': 64,\n",
        "    'save': True,\n",
        "    'train': True\n",
        "}\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args, num_input_features):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        self.fc1 = nn.Linear(num_input_features, self.dim_h)\n",
        "        self.fc2 = nn.Linear(self.dim_h, self.dim_h)\n",
        "        self.fc_mean = nn.Linear(self.dim_h, self.n_z)\n",
        "        self.fc_logvar = nn.Linear(self.dim_h, self.n_z)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        mean = self.fc_mean(x)\n",
        "        logvar = self.fc_logvar(x)\n",
        "        return mean, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args, num_input_features):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dim_h = args['dim_h']\n",
        "        self.n_z = args['n_z']\n",
        "        self.fc1 = nn.Linear(self.n_z, self.dim_h)\n",
        "        self.fc2 = nn.Linear(self.dim_h, self.dim_h)\n",
        "        self.fc_output = nn.Linear(self.dim_h, num_input_features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc_output(x)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_input_features):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_input_features, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "def G_SM1(X, y, n_to_sample, cl, encoder, decoder):\n",
        "    # Genera amostras sintéticas usando o GAN\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "    dataloader = torch.utils.data.DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=args['batch_size'], shuffle=True)\n",
        "\n",
        "    synthetic_data = []\n",
        "    for _ in range(n_to_sample):\n",
        "        z = torch.randn(1, args['n_z'])\n",
        "        synthetic_sample = decoder(z).detach().numpy()\n",
        "        synthetic_data.append(synthetic_sample)\n",
        "\n",
        "    synthetic_data = np.vstack(synthetic_data)\n",
        "    synthetic_labels = np.array([cl] * n_to_sample)\n",
        "    return synthetic_data, synthetic_labels\n",
        "\n",
        "def calculate_n_to_sample(y):\n",
        "    class_counts = np.bincount(y)\n",
        "    major_class_count = np.max(class_counts)\n",
        "    n_classes = len(class_counts)\n",
        "    n_to_sample_dict = {cl: major_class_count - class_counts[cl] for cl in range(n_classes)}\n",
        "    return n_to_sample_dict, major_class_count\n",
        "\n",
        "# Diretório de entrada e saída\n",
        "INPUT_DIR = \"/content/drive/MyDrive/PHD_new/dataset_tratado/fast/\"\n",
        "OUTPUT_DIR_TRAIN = \"/content/drive/MyDrive/PHD_new/resultados/dsto_gan/treino/\"\n",
        "OUTPUT_DIR_TEST = \"/content/drive/MyDrive/PHD_new/resultados/dsto_gan/teste/\"\n",
        "MODELO_DIR = \"/content/drive/MyDrive/PHD_new/resultados/dsto_gan/modelos/\"\n",
        "MODELO_DIR_DST = \"/content/drive/MyDrive/PHD_new/resultados/dsto_gan/modelos_dsto/\"\n",
        "\n",
        "# Verificar se o diretório de saída existe, se não, criar\n",
        "os.makedirs(OUTPUT_DIR_TRAIN, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR_TEST, exist_ok=True)\n",
        "os.makedirs(MODELO_DIR, exist_ok=True)\n",
        "os.makedirs(MODELO_DIR_DST, exist_ok=True)\n",
        "\n",
        "# Listar arquivos .csv no diretório de entrada\n",
        "csv_files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.csv')]\n",
        "\n",
        "# Iterar sobre os arquivos .csv\n",
        "for csv_file in csv_files:\n",
        "    input_path = os.path.join(INPUT_DIR, csv_file)\n",
        "    data = pd.read_csv(input_path)\n",
        "    X = data.drop('class', axis=1).values\n",
        "    y = data['class'].values\n",
        "    print(f\"Processing: {csv_file}\")\n",
        "    print(f\"CLASS DISTRIBUTION:\\n{data['class'].value_counts(normalize=True) * 100}\\n\")\n",
        "\n",
        "    # Dividir os dados em conjunto de treinamento e validação\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "    # Inicializar o DataFrame para armazenar as métricas de avaliação\n",
        "    reports_df_train = pd.DataFrame(columns=['Classifier', 'Class', 'Fold', 'Precision', 'Recall', 'F1-Score'])\n",
        "    reports_df_val = pd.DataFrame(columns=['Classifier', 'Class', 'Precision', 'Recall', 'F1-Score'])\n",
        "\n",
        "\n",
        "    # Define classifiers\n",
        "    classifiers = {\n",
        "        'Decision Tree': DecisionTreeClassifier(),\n",
        "        'Random Forest': RandomForestClassifier(),\n",
        "        'Neural Network': MLPClassifier(),\n",
        "        'KNN': KNeighborsClassifier(),\n",
        "        'XGBoost': XGBClassifier()  # Adicionando XGBClassifier\n",
        "    }\n",
        "\n",
        "    # Inicializar listas para armazenar as métricas de validação de cada classificador\n",
        "    precision_val_list = {classifier_name: [] for classifier_name in classifiers}\n",
        "    recall_val_list = {classifier_name: [] for classifier_name in classifiers}\n",
        "    f1_val_list = {classifier_name: [] for classifier_name in classifiers}\n",
        "\n",
        "    # Realizar validação cruzada estratificada de 10 folds\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_index, test_index) in enumerate(skf.split(X_train, y_train), 1):\n",
        "        X_fold_train, X_fold_test = X_train[train_index], X_train[test_index]\n",
        "        y_fold_train, y_fold_test = y_train[train_index], y_train[test_index]\n",
        "\n",
        "        # Treinar o GAN\n",
        "        if args['train']:\n",
        "            encoder = Encoder(args, X_fold_train.shape[1])\n",
        "            decoder = Decoder(args, X_fold_train.shape[1])\n",
        "            discriminator = Discriminator(X_fold_train.shape[1])\n",
        "            optimizer_g = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=args['lr'])\n",
        "            optimizer_d = optim.Adam(discriminator.parameters(), lr=args['lr'])\n",
        "            criterion_g = nn.MSELoss()\n",
        "            criterion_d = nn.BCELoss()\n",
        "\n",
        "            dataloader = torch.utils.data.DataLoader(torch.tensor(X_fold_train, dtype=torch.float32), batch_size=args['batch_size'], shuffle=True)\n",
        "\n",
        "            for epoch in range(args['epochs']):\n",
        "                for batch in dataloader:\n",
        "                    # Treinar o discriminador\n",
        "                    optimizer_d.zero_grad()\n",
        "                    real_labels = torch.ones(batch.size(0), 1)\n",
        "                    fake_labels = torch.zeros(batch.size(0), 1)\n",
        "\n",
        "                    outputs = discriminator(batch)\n",
        "                    d_loss_real = criterion_d(outputs, real_labels)\n",
        "\n",
        "                    z = torch.randn(batch.size(0), args['n_z'])\n",
        "                    fake_data = decoder(z)\n",
        "                    outputs = discriminator(fake_data.detach())\n",
        "                    d_loss_fake = criterion_d(outputs, fake_labels)\n",
        "\n",
        "                    d_loss = d_loss_real + d_loss_fake\n",
        "                    d_loss.backward()\n",
        "                    optimizer_d.step()\n",
        "\n",
        "                    # Treinar o gerador\n",
        "                    optimizer_g.zero_grad()\n",
        "                    outputs = discriminator(fake_data)\n",
        "                    g_loss = criterion_g(fake_data, batch) + criterion_d(outputs, real_labels)\n",
        "                    g_loss.backward()\n",
        "                    optimizer_g.step()\n",
        "\n",
        "                if args['save']:\n",
        "                    torch.save(encoder.state_dict(), os.path.join(MODELO_DIR_DST, f'encoder_epoch{epoch}.pt'))\n",
        "                    torch.save(decoder.state_dict(), os.path.join(MODELO_DIR_DST, f'decoder_epoch{epoch}.pt'))\n",
        "\n",
        "        encoder.load_state_dict(torch.load(os.path.join(MODELO_DIR_DST, 'encoder_epoch99.pt')))\n",
        "        decoder.load_state_dict(torch.load(os.path.join(MODELO_DIR_DST, 'decoder_epoch99.pt')))\n",
        "\n",
        "        y = y.astype(np.int64)\n",
        "        n_to_sample_dict, major_class_count = calculate_n_to_sample(y)\n",
        "\n",
        "        X_synthetic_list = []\n",
        "        y_synthetic_list = []\n",
        "        for cl, n_samples in n_to_sample_dict.items():\n",
        "            if n_samples > 0:\n",
        "                X_synthetic, y_synthetic = G_SM1(X, y, n_samples, cl, encoder, decoder)\n",
        "                X_synthetic_list.append(X_synthetic)\n",
        "                y_synthetic_list.append(y_synthetic)\n",
        "\n",
        "        if X_synthetic_list:\n",
        "            X_synthetic_combined = np.concatenate(X_synthetic_list, axis=0)\n",
        "            y_synthetic_combined = np.concatenate(y_synthetic_list, axis=0)\n",
        "            X_combined = np.vstack((X, X_synthetic_combined))\n",
        "            y_combined = np.hstack((y, y_synthetic_combined))\n",
        "        else:\n",
        "            X_combined = X\n",
        "            y_combined = y\n",
        "\n",
        "        classifiers = {\n",
        "            'Decision Tree': DecisionTreeClassifier(),\n",
        "            'Random Forest': RandomForestClassifier(),\n",
        "            'Neural Network': MLPClassifier(),\n",
        "            'KNN': KNeighborsClassifier(),\n",
        "            'XGBoost': XGBClassifier()\n",
        "        }\n",
        "\n",
        "\n",
        "        optimized_classifiers = {}\n",
        "        for classifier_name, classifier in classifiers.items():\n",
        "            search = BayesSearchCV(classifier, param_spaces[classifier_name], n_iter=10, cv=3, n_jobs=-1, random_state=42)\n",
        "            search.fit(X_train, y_train)\n",
        "            optimized_classifiers[classifier_name] = search.best_estimator_\n",
        "\n",
        "        for classifier_name, classifier in optimized_classifiers.items():\n",
        "            classifier.fit(X_combined, y_combined)\n",
        "            joblib.dump(classifier, os.path.join(MODELO_DIR, f'{csv_file}_{classifier_name}_fold{fold}_model.pkl'))\n",
        "\n",
        "            y_pred_train = classifier.predict(X_combined)\n",
        "            precision_train = precision_score(y_combined, y_pred_train, average=None)\n",
        "            recall_train = recall_score(y_combined, y_pred_train, average=None)\n",
        "            f1_train = f1_score(y_combined, y_pred_train, average=None)\n",
        "\n",
        "            for class_label, precision, recall, f1 in zip(np.unique(y_combined), precision_train, recall_train, f1_train):\n",
        "                reports_df_train = pd.concat([reports_df_train, pd.DataFrame({\n",
        "                    'Classifier': classifier_name,\n",
        "                    'Class': class_label,\n",
        "                    'Fold': fold,\n",
        "                    'Precision': precision,\n",
        "                    'Recall': recall,\n",
        "                    'F1-Score': f1\n",
        "                }, index=[0])])\n",
        "\n",
        "            y_pred_val = classifier.predict(X_val)\n",
        "            precision_val = precision_score(y_val, y_pred_val, average=None)\n",
        "            recall_val = recall_score(y_val, y_pred_val, average=None)\n",
        "            f1_val = f1_score(y_val, y_pred_val, average=None)\n",
        "\n",
        "            precision_val_list[classifier_name].append(precision_val)\n",
        "            recall_val_list[classifier_name].append(recall_val)\n",
        "            f1_val_list[classifier_name].append(f1_val)\n",
        "\n",
        "    for classifier_name in classifiers:\n",
        "        precision_val_avg = np.mean(precision_val_list[classifier_name], axis=0)\n",
        "        recall_val_avg = np.mean(recall_val_list[classifier_name], axis=0)\n",
        "        f1_val_avg = np.mean(f1_val_list[classifier_name], axis=0)\n",
        "\n",
        "        for class_label, precision, recall, f1 in zip(np.unique(y), precision_val_avg, recall_val_avg, f1_val_avg):\n",
        "            reports_df_val = pd.concat([reports_df_val, pd.DataFrame({\n",
        "                'Classifier': classifier_name,\n",
        "                'Class': class_label,\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'F1-Score': f1\n",
        "            }, index=[0])])\n",
        "\n",
        "    reports_df_train.to_csv(os.path.join(OUTPUT_DIR_TRAIN, f'{csv_file}'), index=False)\n",
        "    reports_df_val.to_csv(os.path.join(OUTPUT_DIR_TEST, f'{csv_file}'), index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKDR+jijL2dJRSywheuUx6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}